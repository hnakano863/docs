'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/learn-docs/docs/','title':"Docs",'content':""});index.add({'id':1,'href':'/learn-docs/docs/%E6%99%82%E7%B3%BB%E5%88%97%E5%88%86%E6%9E%90/state-space-python/','title':"Python による状態空間モデルの実装法",'content':" Python で線形ガウス状態空間モデルを実現するには、統計分析向けライブラリの statsmodels  を用いるとよい。\nstatsmodels での線形ガウス状態空間モデルの式 statsmodels での線形ガウス状態空間モデルの式は、前の記事で導入した式と若干異なるので、注意が必要。実際に statsmodels で採用されている形式での線形ガウス状態空間モデルの式を示す。\n\\[ \\begin{align} y_t \u0026amp;= Z_t \\alpha_t + d_t + \\varepsilon_t, \u0026amp; \\varepsilon_t \\sim \\mathcal{N}(0, H_t) \\\\\n\\alpha_{t+1} \u0026amp;= T_t \\alpha_t + c_t + R_t \\eta_t, \u0026amp; \\eta_t \\sim \\mathcal{N}(0, Q_t) \\\\\n\\end{align} \\]\n第 1 式の、\\(y_t\\) についての式が観測方程式である。\nいくつかのベクトルについては、 statsmodels 固有の名前が与えられている。\n   記号 名称 説明     \\(Z_t\\) design    \\(d_t\\) obs_intercept    \\(H_t\\) obs_cov 観測誤差の共分散   \\(T_t\\) transition    \\(c_t\\) state_intercept    \\(R_t\\) selection    \\(Q_t\\) state_cov 過程誤差の共分散    ローカル線形トレンドモデルの実装 ローカル線形トレンドモデルの式を下記に再掲する。\n\\[ \\begin{align} y_t \u0026amp; = \\mu_t + \\varepsilon_t \\qquad \u0026amp; \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\\\\n\\mu_{t+1} \u0026amp; = \\mu_t + \\nu_t + \\xi_t \u0026amp; \\xi_t \\sim N(0, \\sigma_\\xi^2) \\\\\n\\nu_{t+1} \u0026amp; = \\nu_t + \\zeta_t \u0026amp; \\zeta_t \\sim N(0, \\sigma_\\zeta^2) \\\\\n\\end{align} \\]\nただし、第 1 式が観測方程式、第 3 式がトレンド変化に関する式である。\nstatsmodels では行列を用いた表現を前提とするので、上記ローカル線形トレンドモデルの式を行列を用いて書き直す。\n\\[ \\begin{align} y_t \u0026amp; = \\begin{bmatrix} 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} \\mu_t \\\\\\ \\nu_t \\end{bmatrix} + \\varepsilon_t \\\\\\ \\begin{bmatrix} \\mu_{t+1} \\\\\\ \\nu_{t+1} \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 1 \\\\\\ 0 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mu_t \\\\\\ \\nu_t \\end{bmatrix} + \\begin{bmatrix} \\xi_t \\\\\\ \\zeta_t \\end{bmatrix} \\\\\n\\end{align} \\]\nこのとき、推定すべきパラメタは、\\(\\sigma^2_\\varepsilon\\) , \\(\\sigma^2_\\xi\\) , \\(\\sigma^2_\\zeta\\) である。\nただし、 \\(\\xi_t\\) と \\(\\zeta_t\\) はひとつのベクトルにまとまっているので、 state_cov \\(Q_t\\) は \\((2 \\times 2)\\) の行列になることに注意。\n\\[ \\begin{align} H_t \u0026amp; = \\begin{bmatrix} \\sigma_\\varepsilon^2 \\end{bmatrix} \\\\\nQ_t \u0026amp; = \\begin{bmatrix} \\sigma_\\xi^2 \u0026amp; 0 \\\\\\ 0 \u0026amp; \\sigma_\\zeta^2 \\end{bmatrix} \\end{align} \\]\nモデリングフレームワーク 観測データから、カルマンフィルタと最尤法を用いて状態とパラメタを推定したい。それを実現するには、 statsmodels.tsa.statespace.MLEModel を継承したクラスを作成すればよい。ちなみに MLE は最尤推定(Maximum Likelyhood Estimator)の略。\nimport statsmodels.api as sm class LocalLinearTrend(sm.tsa.statespace.MLEModel): def __init__(self, endog): この新しく作成するモデルには、必ず以下のメソッドを定義する。\n__init__() 観測データを endog として引数にとる。\n__init__() では、モデルの方程式と状態の初期化方法を定義しなければならない。\n  モデル式の定義\nモデル式の定義方法から説明する。\nまず、状態ベクトルの次元数 k_states と、過程誤差ベクトルの次元数 k_posdef を指定する。ローカル線形トレンドモデルの場合、どちらも 2 である。\ndef __init__(self, endog): k_states = k_posdef = 2 次に、状態空間行列を定義する。状態空間行列とは、 \\(Z_t\\) や \\(d_t\\) などの statsmodels で名前のついている行列のことである。デフォルトではすべて零行列となっているので、変更しておきたい部分だけ定義すればよい。\nローカル線形トレンドモデルの場合に定義するのは、以下の表の通り。\n   行列 名前 値     \\(Z_t\\) design \\(\\begin{bmatrix} 1 \u0026amp; 0 \\ \\end{bmatrix}\\)   \\(T_t\\) transition \\(\\begin{bmatrix} 1 \u0026amp; 1 \\\\\\ 0 \u0026amp; 1 \\ \\end{bmatrix}\\)   \\(R_t\\) selection 単位行列    とくに、 selection は単位行列であることが多いのだが、うっかりで定義し忘れやすいので注意。\n# 状態空間行列は self.ssm に格納する。 # ssm は多分 state space matrix の略。 import numpy as np self.ssm[\u0026#39;design\u0026#39;] = np.array([1, 0]) self.ssm[\u0026#39;transition\u0026#39;] = np.array([[1, 1], [0, 1]]) self.ssm[\u0026#39;selection\u0026#39;] = np.eye(k_states) # 2×2 の単位行列    状態の初期化\nカルマンフィルタを実現するためには、\\(t=0\\) のときの状態ベクトルとの平均と分散を定める必要がある。\nMLEModel には、初期化を助けるための関数がそなわっているので、どの関数を選ぶかを指定するだけでよい。\n initialize_known(initial_state, initial_state_cov) : 初期値の確率分布がわかっているときに使う。 initialize_stationary : ARMA などのように、定常状態であることがわかっているときに使う。 initialize_approximate_diffuse : 散漫初期化をする。上記 2 つが適切でないときに使うと考えてよい。   散漫初期化や定常状態の場合\n# 初期化は親クラスの機能を使う super(LocalLinearTrend, self).__init__( endog, k_states=k_states, k_posdef=k_posdef, initialization=\u0026#34;approximate_diffuse\u0026#34; # initialization=\u0026#34;stationary\u0026#34; ) 初期値がわかっているとき\nsuper(LocalLinearTrend, self).__init__( endog, k_states=k_states, k_posdef=k_posdef, initialization=\u0026#34;known\u0026#34;, initial_state=np.array([0,0]) # 初期値の平均ベクトル。 initial_state_cov=np.array([[50, 0], [0, 30]]) # 初期値の共分散行列。 )  update(params) 最尤法を用いてパラメタを推定するときに、 MLEModel がパラメタの更新に用いるメソッド。新たに計算されたパラメタをそれぞれどこに代入すればいいかを設定する。\n@property def param_names(self): \u0026#34;\u0026#34;\u0026#34; 推定するパラメタの名前。 分析結果を表示するときにもこの名前が使われる。 \u0026#34;\u0026#34;\u0026#34; # ローカル線形トレンドモデルで推定するパラメタは、 # 観測誤差の分散/状態変化の誤差の分散/トレンド変化の誤差の分散の三つ return [\u0026#39;var.measurement\u0026#39;, \u0026#39;var.state\u0026#39;, \u0026#39;var.trend\u0026#39;] @property def start_params(self): \u0026#34;\u0026#34;\u0026#34; 3 つのパラメタの初期値。 わりと適当でいいけど、絶対に取り得ない値だけは避ける。 例えば、分散は 0 以下にはならないので、必ず正の値にする。 \u0026#34;\u0026#34;\u0026#34; # 今回は観測値の分散を 3 つのパラメタの初期値とする。 return [np.std(self.endog)] * 3 def update(self, params, *args, **kwargs): # まずは親クラスの結果を受け取る。 params = super(LocalLinearTrend, self).update(params, *args, **kwargs) # パラメタは array-like # 格納順序は`param-names`で決めたとおり。 # 観測誤差の分散 self.ssm[\u0026#39;obs_cov\u0026#39;,0,0] = params[0] # 過程誤差の分散 # `state_cov` は 2×2 の行列で、更新するのはその対角成分だけ。 self.ssm[\u0026#39;state_cov\u0026#39;,0,0] = params[1] self.ssm[\u0026#39;state_cov\u0026#39;,1,1] = params[2] transform/untransform パラメタの推定を行うときに、 statsmodels の最適化アルゴリズムでは、パラメタの値の範囲を制限できない。しかし、推定したいパラメタが分散のように負の値をとらないだとか、なんらかの制約条件をもつことは多い。\nこの問題の解決のために、パラメタをそのまま最適化アルゴリズムで求めるのではなく、何らかの値の変換を介することができるように設計されている。\nローカル線形トレンドモデルの場合、パラメタは全て分散なので、値は負にならない。従って、最適化アルゴリズムで求めた値(=負もとりうる)を 2 乗する。\ndef transform(self, unconstrained): \u0026#34;\u0026#34;\u0026#34; 最適化アルゴリズムの出力値を尤度評価の時に変換する。 unconstrained は np.ndarray \u0026#34;\u0026#34;\u0026#34; # ローカル線形トレンドモデルのパラメタは分散なので負にならない。 # 従って、`unconstrained` を 2 乗する return unconstrained ** 2 def untransform(self, constrained): \u0026#34;\u0026#34;\u0026#34; パラメタを最適化アルゴリズムに渡すときの変換。 constrained は np.ndarray 基本的に transform の逆の演算を行えばよい。 \u0026#34;\u0026#34;\u0026#34; return constrained ** 0.5 まとめ これまでのコードをひとまとめにする。以下がローカル線形トレンドモデルの実装になる。\nimport numpy as np import pandas as pd from scipy.stats import norm import statsmodels.api as sm class LocalLinearTrend(sm.tsa.statespace.MLEModel): \u0026#34;\u0026#34;\u0026#34;ローカル線形トレンドモデル\u0026#34;\u0026#34;\u0026#34; def __init__(self, endog): k_states = k_posdef = 2 # 状態空間の初期化 super(LocalLinearTrend, self).__init__( endog, k_states=k_states, k_posdef=k_posdef, initialization=\u0026#34;approximate_diffuse\u0026#34;, loglikelihood_burn=k_states # 尤度を求めるときに使う ) # 状態空間行列の指定 self.ssm[\u0026#39;design\u0026#39;] = np.array([1,0]) self.ssm[\u0026#39;transition\u0026#39;] = np.array([[1,1], [0,1]]) self.ssm[\u0026#39;selection\u0026#39;] = np.eye(k_states) # 過程誤差のパラメタの更新に使う # np.diag_indices は行列の対角成分を返す関数 self._state_cov_idx = (\u0026#39;state_cov\u0026#39;,) + np.diag_indices(k_posdef) @property def param_names(self): \u0026#34;\u0026#34;\u0026#34;推定するパラメタの名前\u0026#34;\u0026#34;\u0026#34; return [\u0026#39;var.measurement\u0026#39;, \u0026#39;var.level\u0026#39;, \u0026#39;var.trend\u0026#39;] @property def start_params(self): \u0026#34;\u0026#34;\u0026#34;パラメタの初期値\u0026#34;\u0026#34;\u0026#34; return [np.std(self.endog)] * 3 def transform_params(self, unconstrained): \u0026#34;\u0026#34;\u0026#34;最適化 -\u0026gt; 尤度評価の際の変換\u0026#34;\u0026#34;\u0026#34; return unconstrained ** 2 def untrandform_params(self, constrained): \u0026#34;\u0026#34;\u0026#34;尤度評価-\u0026gt;最適化の際の変換\u0026#34;\u0026#34;\u0026#34; returnconstrained ** 0.5 def update(self, params, *args, **kwargs): \u0026#34;\u0026#34;\u0026#34;パラメタの更新\u0026#34;\u0026#34;\u0026#34; params = super(LocalLinearTrend, self).update(params, *args, **kwargs) # 観測誤差の分散 self.ssm[\u0026#39;obs_cov\u0026#39;,0,0] = params[0] # 過程誤差の分散 self.ssm[self._state_cov_idx] = params[1:] サンプルデータ分析 分析の感覚をつかむために、サンプルデータセットに作成したモデルを適用する。\nサンプルには、 Rdatasets の BJsales を使う。\nbjsales = sm.datasets.get_rdataset(\u0026#34;BJsales\u0026#34;, \u0026#34;datasets\u0026#34;) # メタデータ print(bjsales.__doc__) print(\u0026#39;=\u0026#39;*80) # データシェマ bjsales.data.info() print(\u0026#39;=\u0026#39;*80) # データの先頭 5 行 bjsales.data.head()+---------+-----------------+ | BJsales | R Documentation | +---------+-----------------+ Sales Data with Leading Indicator --------------------------------- Description ~~~~~~~~~~~ The sales time series ``BJsales`` and leading indicator ``BJsales.lead`` each contain 150 observations. The objects are of class ``\u0026#34;ts\u0026#34;``. Usage ~~~~~ :: BJsales BJsales.lead Source ~~~~~~ The data are given in Box \u0026amp; Jenkins (1976). Obtained from the Time Series Data Library at http://www-personal.buseco.monash.edu.au/~hyndman/TSDL/ References ~~~~~~~~~~ G. E. P. Box and G. M. Jenkins (1976): *Time Series Analysis, Forecasting and Control*, Holden-Day, San Francisco, p. 537. P. J. Brockwell and R. A. Davis (1991): *Time Series: Theory and Methods*, Second edition, Springer Verlag, NY, pp. 414. ================================================================================ \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 150 entries, 0 to 149 Data columns (total 2 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 time 150 non-null int64 1 value 150 non-null float64 dtypes: float64(1), int64(1) memory usage: 2.5 KB ================================================================================ time value 0 1 200.1 1 2 199.5 2 3 199.4 3 4 198.9 4 5 199.0import matplotlib.pyplot as plt # 必要なのは value のみ ts = bjsales.data[\u0026#39;value\u0026#39;] # どんなデータしているか、グラフで見る。 fig = plt.figure(constrained_layout=True, figsize=(8, 5)) gs = fig.add_gridspec(2,2) ax1 = fig.add_subplot(gs[0,:]) ts.plot(ax=ax1) ax1.set_title(\u0026#39;BJ Sales Data\u0026#39;) ax2 = fig.add_subplot(gs[1,0]) sm.graphics.tsa.plot_acf(ts, ax=ax2) ax2.set_title(\u0026#34;Autocorrelation\u0026#34;) ax3 = fig.add_subplot(gs[1,1]) sm.graphics.tsa.plot_pacf(ts,ax=ax3) ax3.set_title(\u0026#34;Partial Autocorrelation\u0026#34;) plt.show()   これにモデルを適用する\nmodel = LocalLinearTrend(np.log(ts)) result = model.fit() result.summary()\u0026lt;class \u0026#39;statsmodels.iolib.summary.Summary\u0026#39;\u0026gt; \u0026#34;\u0026#34;\u0026#34; Statespace Model Results ============================================================================== Dep. Variable: value No. Observations: 150 Model: LocalLinearTrend Log Likelihood 544.866 Date: Mon, 01 Jun 2020 AIC -1083.732 Time: 17:02:47 BIC -1074.740 Sample: 0 HQIC -1080.079 - 150 Covariance Type: opg =================================================================================== coef std err z P\u0026gt;|z| [0.025 0.975] ----------------------------------------------------------------------------------- var.measurement 2.971e-11 2.98e-06 9.99e-06 1.000 -5.83e-06 5.83e-06 var.level 2.778e-05 7.44e-06 3.735 0.000 1.32e-05 4.24e-05 var.trend 2.296e-06 1.01e-06 2.264 0.024 3.09e-07 4.28e-06 =================================================================================== Ljung-Box (Q): 43.58 Jarque-Bera (JB): 0.54 Prob(Q): 0.32 Prob(JB): 0.76 Heteroskedasticity (H): 0.35 Skew: -0.00 Prob(H) (two-sided): 0.00 Kurtosis: 3.30 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). \u0026#34;\u0026#34;\u0026#34; 予測残差がホワイトノイズに近いほど、よいモデルといえる。\nホワイトノイズは正規分布で、自己相関がゼロで、分散が一定だった。\nLjung-Box 検定(自己相関の検定)と Jarque-Bera 検定(正規性の検定)の結果から、残差は正規分布に従い、かつ、自己相関もないといえる。\nただし、Heteroskedasticity test(分散不均一性検定)の結果を見ると、分散が一定であるとは言えなさそうである。このことから、時間変化する分散をモデルに組込めば更に精度を上げられると考えられる。\n予測残差の分布は plot_diagnostics() で見ることもできる。\nfig = plt.figure(figsize=(8, 6)) result.plot_diagnostics(fig=fig) 予測 predict = result.get_prediction() forecast = result.get_forecast(30) fig, ax = plt.subplots(figsize=(9,6)) # 観測データを黒色×マークで表す ts.plot(style=\u0026#39;.:k\u0026#39;, ax=ax, label=\u0026#39;Observations\u0026#39;) np.exp(predict.predicted_mean).plot(ax=ax, label=\u0026#39;One-step-ahead Prediction\u0026#39;) predict_ci = predict.conf_int(alpha=0.8) predict_index = np.arange(len(predict_ci)) ax.fill_between(predict_index[2:], np.exp(predict_ci).iloc[2:, 0], np.exp(predict_ci).iloc[2:, 1], alpha=0.1) np.exp(forecast.predicted_mean).plot(ax=ax, style=\u0026#39;r\u0026#39;, label=\u0026#39;Forecast\u0026#39;) forecast_ci = forecast.conf_int() forecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci)) ax.fill_between(forecast_index, np.exp(forecast_ci).iloc[:, 0], np.exp(forecast_ci).iloc[:, 1], alpha=0.1) # Cleanup the image ax.set_ylim(150, None) legend = ax.legend(loc=\u0026#39;upper left\u0026#39;, bbox_to_anchor=(0, 1));   ナイーブ予測との比較 このモデルを使うことに価値があるかどうかを考えてみる。最も単純なランダムウォークと比較する。\nmodel_rwalk = sm.tsa.UnobservedComponents(np.log(ts), level=\u0026#39;rwalk\u0026#39;) res_rwalk = model_rwalk.fit() res_rwalk.summary()\u0026lt;class \u0026#39;statsmodels.iolib.summary.Summary\u0026#39;\u0026gt; \u0026#34;\u0026#34;\u0026#34; Unobserved Components Results ============================================================================== Dep. Variable: value No. Observations: 150 Model: random walk Log Likelihood 535.509 Date: Mon, 01 Jun 2020 AIC -1069.017 Time: 17:02:50 BIC -1066.013 Sample: 0 HQIC -1067.797 - 150 Covariance Type: opg ================================================================================ coef std err z P\u0026gt;|z| [0.025 0.975] -------------------------------------------------------------------------------- sigma2.level 4.418e-05 4.31e-06 10.241 0.000 3.57e-05 5.26e-05 =================================================================================== Ljung-Box (Q): 117.00 Jarque-Bera (JB): 6.26 Prob(Q): 0.00 Prob(JB): 0.04 Heteroskedasticity (H): 0.34 Skew: 0.45 Prob(H) (two-sided): 0.00 Kurtosis: 3.44 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). \u0026#34;\u0026#34;\u0026#34; まず、残差に自己相関が残っており、分布も正規分布でないことがすぐにわかる。また、AIC()を比較すると\nprint(\u0026#34;LocalLinearTrend: {:.6}\u0026#34;.format(result.aic)) print(\u0026#34;RandomWalk: {:.6}\u0026#34;.format(res_rwalk.aic))LocalLinearTrend: -1083.73 RandomWalk: -1069.02 となり、ローカル線形トレンドモデルのほうが低い。すなわち、少なくともランダムウォークよりはローカル線形トレンドモデルのほうがよいモデルであるとわかる。\n"});index.add({'id':2,'href':'/learn-docs/docs/%E6%99%82%E7%B3%BB%E5%88%97%E5%88%86%E6%9E%90/','title':"時系列分析",'content':" 時系列データとは 一定の時間間隔でとられたデータのことを時系列データという。\n記号法 時点 \\(t\\) におけるデータを \\(y_t\\) と書く。時点 \\(t\\) から \\(k\\) 時点前のデータを \\(y_{t-k}\\) と書く。特に、1時点前のデータは \\(y_{t-1}\\) と書く。\n\\(T\\) 時点までの一連の時系列データをまとめて \\(Y_T\\) のように書く。\n\\[ Y_T = \\{y\\}^T_{t=1} = \\{y_1, y_2, \\cdots ,y_T\\} \\]\nデータ生成過程(DGP : Data Generating Process) 時間経過に従って変化する確率分布のことをデータ生成過程という。データ生成過程のことを単に確率過程ということもある。時系列データはデータ生成過程の一つの実現値であると考えられる。\n各時点のデータから、そのデータを生み出すもととなったデータ生成過程を推測することを、モデリングという。\n平均 時刻 \\(t\\) におけるデータの平均を \\(\\mu_t = \\mathrm{E}[y_t]\\) と表す\n分散 時刻 \\(t\\) でにおけるデータの分散を \\(\\mathrm{Var}[y_t] = \\mathrm{E}[(y_t - \\mu_t)^2]\\) と表す。\n時系列データの構造 時系列データは以下の式のような構造をもつ\n*時系列データ = 短期の自己相関 + 周期成分 + トレンド + 外因性 + ホワイトノイズ*\n自己相関 自己相関とは、異なる時点のデータ同士の相関である。時系列データは前後のデータと相関をもつことが多い。\n例えば、お風呂の温度の変化を考えよう。前の時点でのお湯の温度が 42℃なら、次の時点の温度もだいたい 42℃程度と考えられる。一方、前の時点の温度が 30℃であったとして、次の時点の温度がいきなり 40℃まで上がることは考えにくい。すなわち、この例では、前時点の温度が高いほど次の時点での温度が高くなると考えることができる。\nこのように前後のデータ間に存在する相関関係のことを自己相関という。\n数式表現 以上で説明した自己相関を、数式を用いて定義する。\nまず、自己共分散を定義する\n  自己共分散\n\\(k\\) 時点前のデータとの共分散を \\(k\\) 次の自己共分散といい、以下の式で定義する。\n\\[ \\gamma_{kt} = \\mathrm{Cov}[y_t,y_{t-k}] = \\mathrm{E}[(y_t - \\mu_t)(y_{t-k}-\\mu_{t-k})] \\]\n    自己相関\n自己共分散を標準化したものを自己相関として定義する。\n\\(k\\) 次の自己相関は以下の式で表される。\n\\[ \\rho_{kt} = \\mathrm{Corr}[y_t,y_{t-k}] = \\cfrac{\\mathrm{Cov}[y_t,y_{t-k}]}{\\sqrt{\\mathrm{Var}[y_t]\\mathrm{Var}[y_{t-k}]}} \\]\n自己相関は標準化されているので、その絶対値 \\(|\\rho_{kt}|\\) は 1 未満になる。\n  偏自己相関 \\(k\\) 次の自己相関から、 \\(k-1\\) 次までの自己相関の影響を除いたものを、偏自己相関という。\n偏自己相関の概念について理解を深めるために、ちょっとした計算をしてみよう。\nいま、1時点前のデータとの自己相関が 0.8 の時系列データがあるとする。すなわち、時点 \\(t\\) におけるデータ \\(y_t\\) は、 1 時点前のデータ \\(y_{t-1}\\) を用いて\n\\[y_t = 0.8y_{t-1}\\]\nと表せる。\nこのとき、1時点前のデータと 2 時点前のデータとの間にも同様の相関関係があるはずなので、\n\\[ y_{t-1} = 0.8y_{t-2} \\]\nが成り立つ。\n二つの数式から合わせて \\(y_t{t-1}\\) を消去することで、 \\(y_t\\) と \\(y_{t-2}\\) との相関関係が導き出せる。\n\\[ y_{t} = 0.8 (0.8 y_{t-2}) = 0.64 y_{t-2} \\]\nこのように、2時点前との関係性については何も定義していなかったにも関わらず、1時点前との関係が 2 時点前との関係にまで波及してしまう。\nこのような波及する相関関係の影響を除いて\\(y_t\\) と \\(y_{t-k}\\) との相関関係を表すのが \\(k\\) 次の偏自己相関である。\n\\(k\\) 次の偏自己相関の定義式は以下の通り\n\\[ P_{tk} = \\cfrac{\\mathrm{Cov}[y_t-\\hat{y}_t,y_{t-k}-\\hat{y}_{t-k}]}{\\sqrt{\\mathrm{Var}[y_t-\\hat{y}_t]\\mathrm{Var}[y_{t-k}-\\hat{y}_{t-k}]}} \\]\nただし、 \\(\\hat{y}_t\\) は \\(t\\) 時点における \\(y_t\\) の推定値。\nコレログラム 何時点前との自己相関が強いか調べるために、横軸にラグを、縦軸に相関係数を取ったグラフをコレログラムという。\nPython を使ってコレログラムがどんなものか見てみよう。\nまず、疑似データを作成する。\nimport statsmodels.api as sm from statsmodels.tsa.arima_process import ArmaProcess import matplotlib.pyplot as plt # 疑似データを作成 # y[t] = 0.8 * y[t-1]のプロセスを作成 model = ArmaProcess(ar=[1, - 0.8], ma=[1]) # 200 サンプル作成 samples = model.generate_sample(200) plt.plot(samples) plt.show()   まずは自己相関のコレログラム\nsm.graphics.tsa.plot_acf(samples) plt.show()   縦軸が相関係数、横軸がデータ同士のラグの大きさである。ラグ 0 で相関係数 1 (自分自身との相関は 1)であり、ラグ 1 のとき相関係数 0.8 であることがわかる。しかし、実際には相関していないはずのラグ 2 以上のデータとも相関関係が見える。\nそれでは、本当に相関関係があるかどうか、偏自己相関のコレログラムを見てみよう。\nsm.graphics.tsa.plot_pacf(samples) plt.show()   偏自己相関のコレログラムを見れば、真実のところラグ 2 以上では相関関係がないということがわかる。\n周期成分 ホワイトノイズ 未来を予測するための情報を含まない純粋な雑音のことをホワイトノイズという。ホワイトノイズは \\(\\varepsilon_t\\) という記号で表されることが多い。\nより厳密にホワイトノイズのみたす性質を述べると、以下の 3 つである。\n 期待値が 0 分散が時間に寄らず一定 ラグが 1 以上の自己相関が 0  ホワイトノイズの確率分布として、平均 0, 分散 \\(\\sigma^2\\) の正規分布がしばしば仮定される。\nまた、 \\(\\varepsilon_t\\) がホワイトノイズに従うことを明示するために、\n\\[ \\varepsilon_t \\sim \\mathrm{W.N.}(\\sigma^2) \\]\nと書くことがある。\nホワイトノイズが実際どのような見た目をしているのかというと、\nimport numpy as np # 分散 4 のホワイトノイズを 200 サンプル作成 white_noise = np.random.normal(0, 4, size=200) plt.plot(white_noise) plt.show()   実際、このホワイトノイズの自己相関を確認しておこう。\nsm.graphics.tsa.plot_acf(white_noise) トレンド 時系列データの値が全体的に上昇したり下降したりするとき、トレンドをもつということがある。しかし、トレンドとはなんだろう。数式で表せばどうなるのか。\n以下では、それを見る。\niid 系列 互いに相関がないことを独立という。データが互いに独立で、しかも同じ確率分布から生成されている場合、これらを独立同分布(i.i.d.)なデータという。そして、i.i.d.なデータからなる時系列データを iid 系列という。\n平均 \\(\\mu\\) 分散 \\(\\sigma^2\\) の iid 系列を\n\\[ y_t \\sim \\mathrm{iid}(\\mu,\\sigma) \\]\nと表すことがある。\nランダムウォーク iid 系列の累積和からなる系列をランダムウォークという。数式で表すと、\n\\[ y_t = y_{t-1} + \\epsilon_t, \\,\\,\\,\\,\\, \\epsilon_t \\sim \\mathrm{iid}(0, \\sigma^2) \\]\n例えば、ホワイトノイズの累積和はランダムウォークの一つである。ランダムウォークはこんな見た目をしている。\nplt.plot(white_noise.cumsum()) plt.show()   ランダムウォークの見た目は、データの生成のたびに大きくかわる。\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8, 5.5), sharex=True, sharey=True) for i in range(2): for j in range(2): wht_nz = np.random.normal(0, 4, size=100) ax[i,j].plot(wht_nz.cumsum()) plt.show()   ドリフト率 例えば、1時点進むごとに 2 ずつ増える時系列データを考えよう。そのようなデータは数式で\n\\[ y_t = y_{t-1} + 2 \\]\nと表せる。一般的に、\n\\[ y_t = y_{t-1} + \\delta \\]\nは時点ごとに \\(\\delta\\) ずつ増えていく。こういう時系列データを線形トレンドという。\nこれにさらにホワイトノイズ \\(\\varepsilon_t\\) をのせたものを「ドリフト率 \\(\\delta\\) の確率的トレンド」という\n\\[ y_t = y_{t-1} + \\delta + \\varepsilon_t\\]\nJulia を使って確率的トレンドをグラフにしてみよう。\nusing Plots using Distributions gr() ndist = Normal(0, 4) # ホワイトノイズの確率分布 delt = 2 # ドリフト率 # 配列を初期化 trends = zeros(200) # 初期値をセット trends[1] = 0 # データ作成 for i in 1:(length(trends)-1) trends[i+1] = trends[i] + delt + rand(ndist) end plot(trends)   外因性 外部要因による影響のこと。\n"});index.add({'id':3,'href':'/learn-docs/docs/%E6%99%82%E7%B3%BB%E5%88%97%E5%88%86%E6%9E%90/state-space-model/','title':"状態空間モデル",'content':" 状態空間モデルの構成 状態空間モデルとは、直接観測されない「状態」の存在を仮定したモデルである。\n基本的なワークフローは、\n 方程式によるデータの表現 統計手法を用いた状態/パラメタの推定  の 2 部に分かれる。\n方程式によるデータ表現 状態空間モデルでは、目に見えない状態変化と状態から得られる観測結果を 2 つの異なる方程式に分けて表現する。\n 目に見えない状態の変化 : 状態方程式 状態から得られる観測結果 : 観測方程式  状態方程式 状態変化のプロセスを記述する方程式のことを状態方程式という。\n状態方程式の概念式は、\n{状態} = {前時点の状態を用いた予測値} + {過程誤差}\nで表される。\n観測方程式 状態から観測値を得るプロセスを記述する方程式を観測方程式という。\n観測方程式の概念式は、\n{観測値} = {状態} + {観測誤差}\nで表される。\n線形ガウス状態空間モデル 一次方程式でモデルが表現可能で、なおかつ誤差が正規分布に従うモデルのことを、線形ガウス状態空間モデルという。\n状態空間モデルそのものはかなり幅広い概念だが、時系列データ分析の初歩では、大抵の状態空間モデルは線形ガウス状態空間モデルを仮定している。\n線形ガウス状態空間モデルの場合は、状態の推定とモデルのパラメタの推定を分けて行うことができる。\n 状態の推定: カルマンフィルタを使う。 モデルのパラメタの推定: 最尤法を使う。  この方法は、計算コストや実装コストが低く、ストリーム処理が可能で、オンライン予測に適している。\n以下では、線形ガウス状態空間モデルの概略を述べる。\n状態方程式と観測方程式 状態方程式 時点 \\(t\\) での状態を、\\(\\bm{\\mu}_t\\) とおく。ただし、 \\(\\bm{\\mu}_t\\) は \\(D\\) 次元ベクトルとする。\nこのとき、線形ガウス状態空間モデルにおける状態方程式は、\n\\[\\begin{align} \\bm{\\mu}_t \u0026amp;= T_t\\bm{\\mu}_{t-1} + R_t\\bm{\\eta}_t, \u0026amp; \\bm{\\eta}_t \\sim \\mathcal{N}(0, Q_t) \\end{align}\\]\nとなる。\nただし、記号法は以下の通りである。\n   変数 説明     \\(T_t\\) 状態 \\(\\bm{\\mu}_{t-1}\\) から \\(\\bm{\\mu}_t\\) へ移るときの状態変化を記述する行列。   \\(\\bm{\\eta}_t\\) 状態変化に伴う過程誤差のベクトル。 平均ゼロの正規分布に従う。   \\(R_t\\) 誤差 \\(\\bm{\\eta}_t\\) が状態へおよぼす影響を記述する行列。   \\(Q_t\\) 誤差 \\(\\bm{\\eta}_t\\) が従う正規分布の共分散行列。    例えば、状態が ランダムウォーク である場合は、 \\(T_t\\) , \\(R_t\\) はともに \\(D\\) 次単位行列として、\n\\[\\begin{align} \\bm{\\mu}_t \u0026amp;= \\bm{\\mu}_{t-1} + \\bm{\\eta}_t, \u0026amp; \\bm{\\eta}_t \\sim \\mathcal{N}(0, Q_t) \\end{align}\\]\nとなる。\n(参考)三次元空間上のランダムウォーク\n  観測方程式 時点 \\(t\\) での観測値を、\\(\\bm{y}_t\\) とおく。ただし、 \\(\\bm{y}_t\\) は \\(E\\) 次元ベクトルとする。\nこのとき、線形ガウス状態空間モデルにおける観測方程式は、\n\\[\\begin{align} \\bm{y}_t \u0026amp;= Z_t\\bm{\\mu}_t + \\bm{\\varepsilon}_t, \u0026amp; \\bm{\\varepsilon}_t \\sim \\mathcal{N}(0, H_t) \\end{align}\\]\nとなる。\nただし、記号法は以下の通りである。\n   変数 説明     \\(Z_t\\) 状態 \\(\\bm{\\mu}_t\\) を観測する過程を記述する行列。   \\(\\bm{\\varepsilon}_t\\) 観測誤差のベクトル。 平均ゼロの正規分布に従う。   \\(H_t\\) 誤差 \\(\\bm{\\varepsilon}_t\\) が従う正規分布の共分散行列。    例えば、状態をそのまま観測する場合は、 \\(Z_t\\) は単位行列となる。従って観測方程式は、\n\\[\\begin{align} \\bm{y}_t \u0026amp;= \\bm{\\mu}_t + \\bm{\\varepsilon}_t, \u0026amp; \\bm{\\varepsilon}_t \\sim \\mathcal{N}(0, H_t) \\end{align}\\]\nとなる。\nフィルタリング 観測値を用いて状態の予測値を補正することをフィルタリングという。フィルタリングの流れは以下の図の通り\n  状態方程式/観測方程式によるデータ表現 線形ガウス状態空間モデルの仲間のうち、いくつかの名前のついたモデルを以下で見ていく。これらを見ることによって、状態方程式と観測方程式による表現の方法について慣れてほしい。\n線形回帰モデル 説明変数のないモデル 説明変数のない線形回帰モデルは最も簡単なモデルといえる。以下の式で表される。\n\\[\\begin{align}y_t \u0026amp;= \\alpha + v_t \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma^2) \\end{align} \\]\nこれを状態方程式と観測方程式に分割するならば、\n\\[ \\left\\{ \\begin{align} \\mu_t \u0026amp;= \\alpha \\\\\ny_t \u0026amp;= \\mu_t + v_t \\end{align} \\right. \\]\nこれは、状態は時刻に寄らず一定であり、分散一定の観測誤差だけがあるモデルとして解釈できる。\n1 次の自己回帰モデル 一次の自己回帰モデル AR(1)の式は、\n\\[ \\begin{align} y_t \u0026amp;= c + \\phi_1 y_{t-1} + w_t , \u0026amp; w_t \\sim \\mathcal{N}(0,\\sigma^2) \\end{align} \\]\nと表される。\nこれを状態方程式と観測方程式を用いて表現すると、\n\\[ \\left\\{ \\begin{align} \\mu_t \u0026amp;= c + \\phi_1 \\mu_{t-1} + w_t \\\\\\ y_t \u0026amp;= \\mu_t \\end{align} \\right. \\]\nとなる。\nAR モデルを状態空間モデルとして表すばあい、モデルがまるまる状態方程式に入る。\nAR モデルは観測誤差がないものとしてモデリングされていることに注意。\nローカルレベルモデル 過程誤差と観測誤差をともに含むモデルで、最も簡単な状態空間モデルのひとつ。以下の方程式で表される。\n\\[ \\left\\{ \\begin{align} \\mu_t \u0026amp;= \\mu_{t-1} + w_t, \u0026amp; w_t \\sim \\mathcal{N}(0, \\sigma_w^2) \\\\\ny_t \u0026amp;= \\mu_t + v_t, \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma_v^2) \\\\\n\\end{align} \\right. \\]\n状態方程式のみを見るとランダムウォーク系列である。従って、ローカルレベルモデルのことをランダムウォーク+ノイズモデルということがある。\nローカルレベルモデルの疑似データとコレログラム\n  期待値 時点 \\(t-1\\) までの状態がわかっているものとして、時点 \\(t\\) における \\(\\mu_t\\) の期待値を考える。過程誤差および観測誤差は平均 0 の正規分布に従うため、期待値をとると消える。従って、\n\\[ \\mathrm{E}[\\mu_t|\\mu_{t-1}] = \\mu_{t-1} \\]\nすなわち、状態は時刻に寄らず一定であると考えられる。つまり、もしもローカルレベルモデルを使って予測を行うのなら、予測値は前の値をそのまま返すだけのものとなる。それゆえ、このモデルそのものは予測に役立つものではない。\nARIMA モデルとの関係 ローカルレベルモデルにおいて、観測値の差分系列をとってみると、\n\\[ \\begin{align} \\Delta y_t \u0026amp;= y_t - y_{t-1} \\\\\n\u0026amp;= (\\mu_t + v_t) - (\\mu_{t-1} + v_{t-1}) \\\\\n\u0026amp;= (\\mu_{t-1} + w_t + v_t) - (\\mu_{t-1} + v_{t-1}) \\\\\n\u0026amp;= w_t + v_t - v_{t-1} \\ \\end{align} \\]\nすなわち、ローカルレベルモデルの差分系列は 1 次の移動平均モデル MA(1)で表せる。このことから、ローカルレベルモデルそれ自体は、ARIMA(0,1,1)と等価である。\nローカル線形トレンドモデル ローカルレベルモデルの状態変化に時間変化するトレンドを加えたものを、ローカル線形トレンドモデルという。状態方程式と観測方程式は以下のとおり。\n\\[ \\left\\{ \\begin{align} \\delta_t \u0026amp;= \\delta_{t-1} + \\zeta_t, \u0026amp; \\zeta_t \\sim \\mathcal{N}(0,\\sigma_{\\zeta}^2) \\\\\n\\mu_t \u0026amp;= \\mu_{t-1} + \\delta_{t-1} + w_t, \u0026amp; w_t \\sim \\mathcal{N}(0, \\sigma_w^2) \\\\\ny_t \u0026amp;= \\mu_t + v_t, \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma_v^2) \\\\\n\\end{align} \\right. \\]\n線形回帰モデルとの比較 ローカル線形トレンドモデルへの理解を深めるために、線形回帰モデルと比較する。線形回帰モデルの方程式は、\n\\[ \\begin{align} y_t \u0026amp;= \\alpha + \\beta t + v_t , \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma_v^2) \\end{align} \\]\nである。\nこれをローカル線形トレンドモデルの方程式に従って状態方程式と観測方程式の形に直すと、\n\\[ \\left\\{ \\begin{align} \\delta_t \u0026amp;= \\beta \\\\\n\\mu_t \u0026amp;= \\mu_{t-1} + \\delta_{t-1} \\\\\ny_t \u0026amp;= \\mu_t + v_t, \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma_v^2) \\\\\n\\end{align} \\right. \\]\nとなる。ただし、 \\(\\mu_0 = \\alpha\\) とする。\nローカル線形トレンドモデルと線形回帰モデルの方程式を比較することにより、ローカル線形トレンドモデルの特徴として以下の 2 点がわかる。\n トレンドの値が時間変化しうる。 過程誤差を含む。  行列による表現 ローカル線形トレンドモデルは線形ガウス状態空間モデルの一つであるから、行列を用いて以下のように線形ガウス状態空間モデルの一般形にあてはめて表現できる。\n\\[ \\left\\{ \\begin{align} \\begin{bmatrix} \\mu_t \\ \\delta_t \\ \\end{bmatrix} \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 1 \\\\\\ 0 \u0026amp; 1 \\\\\n\\end{bmatrix} \\begin{bmatrix} \\mu_{t-1} \\\\\\ \\delta_{t-1} \\ \\end{bmatrix} + \\bm{\\eta}_t \\\\\ny_t \u0026amp;= \\begin{bmatrix} 1 \u0026amp; 0 \\ \\end{bmatrix} \\begin{bmatrix} \\mu_t \\\\\\ \\delta_t \\ \\end{bmatrix} + v_t \\\\\n\\end{align} \\right. \\]\nただし、 \\(\\bm{\\eta}_t \\sim \\mathcal{N}(0, Q_t)\\) , \\(v_t \\sim \\mathcal{N}(0, \\sigma_v^2)\\) である。\n\\(Q_t\\) は過程誤差の共分散行列で、\n\\[ Q_t = \\begin{bmatrix} \\sigma_{\\zeta}^2 \u0026amp; 0 \\\\\\ 0 \u0026amp; \\sigma_w^2 \\end{bmatrix} \\]\n周期変動のモデル化 ダミー変数の利用 ダミー変数を導入することにより、周期的変動をモデリングできる。たとえば、頻度 4 の季節変動があるばあい、\n\\[ \\left\\{ \\begin{align} \\gamma_{1,t} \u0026amp;= -\\gamma_{1,t-1}-\\gamma_{2,t-1}-\\gamma_{3,t-1} + \\eta_t \\,, \u0026amp; \\eta_t \\sim \\mathcal{N}(0, \\sigma_{\\eta}^2) \\\\\n\\gamma_{2,t} \u0026amp;= \\gamma_{1,t-1} \u0026amp; \\\\\n\\gamma_{3,t} \u0026amp;= \\gamma_{2,t-1} \u0026amp; \\\\\ny_t \u0026amp;= \\gamma_{1,t} + v_t \\,, \u0026amp; v_t \\sim \\mathcal{N}(0,\\sigma_v^2) \\\\\n\\end{align} \\right. \\]\nと表すことができる。\nこれがきちんと周期をモデリングできていることを確認するために、以下で \\(\\gamma_{1, t+1}\\) を計算する。(簡単のため、以下では過程誤差 \\(\\xi_t\\) を無視する)\n第一式で \\(t=t+1\\) とすると、\n\\[ \\gamma_{1, t+1} = -\\gamma_{1,t}-\\gamma_{2,t}-\\gamma_{3,t} \\]\nこれに \\(t=t\\) における結果を適用して \\(\\gamma_{1,t}\\) , \\(\\gamma_{2,t}\\) , \\(\\gamma_{3, t}\\) を消去すると、\n\\[ \\gamma_{1,t+1} = -(-\\gamma_{1,t-1}-\\gamma_{2,t-1}-\\gamma_{3,t-1})-\\gamma_{1,t-1}-\\gamma_{2,t-1} \\]\n整理して、\n\\[ \\gamma_{1, t+1} = \\gamma_{3, t-1} \\]\nしたがって、 \\(\\gamma_{1, t}\\) を \\(t=0\\) から順に並べていくと、\n\\[ \\gamma_{1, 0}, \\ -(\\gamma_{1, 0}+\\gamma_{2,0}+\\gamma_{3,0}),\\ \\gamma_{3, 0},\\ \\gamma_{2, 0}, \\ \\gamma_{1, 0},\\ \\ldots \\]\nというふうにローテーションする。従って、頻度 4 の季節変動が表現されている\n周期関数の利用 ここでは詳しく述べないが、 \\(\\sin\\) , \\(\\cos\\) などの周期関数を用いて周期成分をモデリングすることもある\n基本構造時系列モデル トレンド、周期変動、ホワイトノイズの和で表される時系列データのことを、 基本構造時系列モデル という\n基本構造時系列モデルは状態空間モデルを用いて表すことができる。\nローカル線形トレンドモデルに季節項を入れたものと考えてよい。観測方程式のみ示す。\n\\[ y_t = \\mu_t + \\gamma_t + v_t \\]\nただし、 \\(\\mu_t\\) はトレンド成分, \\(\\gamma_t\\) は季節成分を表す\n時変係数モデル ARIMAX のように、外生変数(回帰変数)を含むモデルを構築できる。これにより、異常値の補正などが可能となる。\nまた、線形回帰モデルや ARIMAX とは異なり、外生変数が時間変化するモデルを作成できる。\n例えば、ローカルレベルモデルに時変係数を入れたモデルは、\n\\[ \\left\\{ \\begin{align} \\beta_t \u0026amp;= \\beta_{t-1} + \\tau_t \\\\\n\\mu_t \u0026amp;= \\mu_{t-1} + w_t \\\\\ny_t \u0026amp;= \\mu_t + \\beta_t \\psi_t + v_t \\\\\n\\end{align} \\right. \\]\nただし、 \\(\\tau_t\\) , \\(w_t\\) , \\(v_t\\) は誤差項で、 \\(\\psi_t\\) は外生変数、 \\(\\beta_t\\) は時変係数とする。\nカルマンフィルタによる推定法 基本の流れを習得するために、ローカルレベルモデルを前提とする。\nカルマンフィルタの基本の流れ\n 1 時点先の状態の予測 観測値を用いての状態の補正  ローカルレベルモデルでは予測は前の時点と同じなので、ここでは補正の方法を扱う。補正前と補正後との関係式は、\n{補正後の状態} = {補正前の状態} + {カルマンゲイン} * {予測残差}\n補正後の状態のことを フィルタ化推定量 という。\nカルマンゲイン 補正の大きさを制御する係数のことを、カルマンゲインという。\nカルマンゲインのアイディア\n 状態の予測誤差が大きいとき: 状態の予測は外れやすいので、補正は大きくすべき。 観測誤差が大きいとき: 観測値は信頼できないので、観測値による補正量は小くするべき。  カルマンゲインの定義の概念式\n{カルマンゲイン} = {状態の予測誤差の分散} / ( {状態の予測誤差の分散} + {観測誤差の分散} )\nローカルレベルモデルでのカルマンフィルタ 記号法 以下に、ローカルレベルモデルの式を再掲する。\n\\[ \\left\\{ \\begin{align} \\mu_t \u0026amp;= \\mu_{t-1} + w_t, \u0026amp; w_t \\sim \\mathcal{N}(0, \\sigma_w^2) \\\\\ny_t \u0026amp;= \\mu_t + v_t , \u0026amp; v_t \\sim \\mathcal{N}(0, \\sigma_v^2) \\\\\n\\end{align} \\right. \\]\n以下、いくつか記号を導入する。特に明記しなければ、今後も同様の記号を用いる。\n 時点 \\(t\\) までの全ての観測値 \\(\\{y_1, \\cdots ,y_t\\}\\) を \\(Y_t\\) とする。 時点 \\(t\\) における観測値 \\(y_t\\) の予測値を \\(\\hat{y}_t\\) とする。 時点 \\(t\\) における状態 \\(\\mu_t\\) の予測値を \\(\\hat{\\mu}_t\\) とする。予測値 \\(\\hat{\\mu}_t\\) は、前時点までの観測値 \\(Y_{t-1}\\) が得られている条件のもとで、状態方程式の期待値をとることで求める。従って、\n\\[ \\hat{\\mu}_t = \\mathrm{E}[\\mu_t|Y_{t-1}] \\]\n 状態の予測値 \\(\\hat{\\mu}_t\\) を時点 \\(t\\) までの観測値 \\(Y_t\\) で補正したものを、フィルタ化推定量と呼び、\\(\\mu_{t|t}\\) であらわす。フィルタ化推定量は \\(Y_t\\) が得られた条件のもとでの \\(\\mu_t\\) の期待値である。従って、\n\\[ \\mu_{t|t} = \\mathrm{E}[\\mu_t|Y_t] \\]\n 時点 \\(t\\) における状態 \\(\\mu_{t}\\) の予測誤差の分散を \\(P_t\\) で表す。\n\\(P_t\\) は、 \\(Y_{t-1}\\) が与えられた条件のもとの \\(\\mu_t\\) の分散である。従って、\n\\[ P_t = \\mathrm{Var}[\\mu_t|Y_{t-1}] \\]\n 時点 \\(t\\) でのフィルタ化推定量 \\(\\mu_{t|t}\\) の推定誤差の分散を \\(P_{t|t}\\) で表す。\n\\(P_{t|t}\\) は、 \\(Y_t\\) が与えられた条件のもとでの状態 \\(\\mu_t\\) の分散である。従って、\n\\[ P_{t|t} = \\mathrm{Var}[\\mu_t|Y_t] \\]\n 時点 \\(t\\) での観測値 \\(y_t\\) の予測誤差の分散を \\(F_t\\) で表す。\n 時点 \\(t\\) でのカルマンゲインを \\(K_t\\) で表す。\n  カルマンフィルタの計算の流れ まず、時点 \\(t\\) における状態の予測値 \\(\\hat{\\mu}_t\\) を求める。ローカルレベルモデルでは、状態変化は起こらないので、前時点の状態をそのまま用いればよい。このとき、前時点の状態はすでに観測値によって補正されたフィルタ化推定量であることに注意する。従って、\n\\[ \\hat{\\mu}_t = \\mu_{t-1|t-1} \\]\n次に、状態の予測誤差の分散 \\(P_t\\) を求める。\n状態が \\(t\\) 時点にうつるのに伴って、過程誤差 \\(w_t\\) の大きさの分だけ状態の予測誤差が大きくなる。従って、\n\\[ P_t = P_{t-1|t-1} + \\sigma_w^2 \\]\n次に、観測値の予測値 \\(\\hat{y}_t\\) を求める。\nローカルレベルモデルでは、観測値と状態の予測値は等しいので、\n\\[ \\hat{y}_t = \\hat{\\mu}_t \\]\n観測値の予測誤差の分散 \\(F_t\\) を求める。\n観測値の予測誤差は、状態の予測誤差に観測誤差が加わるので、\n\\[ F_t = P_t + \\sigma_v^2 \\]\nカルマンゲイン \\(K_t\\) を求める。\n{カルマンゲイン} = {状態の予測誤差の分散} / ( {状態の予測誤差の分散} + {観測誤差の分散} )なので、\n\\[ K_t = \\frac{P_t}{P_t + \\sigma_v^2} = \\frac{P_t}{F_t} \\]\n従って、時刻 \\(t\\) におけるフィルタ化推定量 \\(\\mu_{t|t}\\) は、\n\\[ \\mu_{t|t} = \\hat{\\mu}_t + K_t (y_t - \\hat{y}_t) \\]\nとなる。\nまた、状態の予測誤差のフィルタ化推定量 \\(P_{t|t}\\) も以下の式で求められる。\n\\[ P_{t|t} = (1-K_t)P_t \\]\nカルマンフィルタのまとめ   状態の予測\n\\[\\begin{align} \\hat{\\mu}_t \u0026amp;= \\mu_{t-1|t-1} \\\\\nP_t \u0026amp;= P_{t-1|t-1} + \\sigma_w^2 \\\\\n\\end{align} \\]\n    観測値の予測\n\\[ \\begin{align} \\hat{y}_t \u0026amp;= \\hat{\\mu}_t \\\\\nF_t \u0026amp;= P_t + \\sigma_v^2 \\\\\n\\end{align} \\]\n    カルマンゲイン\n\\[ K_t = \\frac{P_t}{F_t} \\]\n    状態の補正\n\\[ \\begin{align} \\mu_{t|t} \u0026amp;= \\hat{\\mu}_t + K_t(y_t - \\hat{y}_t) \\\\\nP_{t|t} \u0026amp;= (1-K_t)P_t \\\\\n\\end{align} \\]\n  カルマンフィルタの問題点 状態の予測を行うためには、前期の状態が必要になる。ところが、一番最初は前期の状態が存在しないので、 \\(\\mu_0\\) と \\(P_0\\) に適当な初期値を設定しなければならない。この初期値に AIC などの情報量基準も依存してしまうので、できれば適当に決めることなしに解決する必要がある。\n散漫カルマンフィルタ カルマンフィルタの初期値の問題を解決する方法。\n\\(P_0=\\infty\\) とする。(この初期化法を 散漫初期化 という)\nこれによって、ローカルレベルモデルの場合は以下のように $\u0026mu;_0$が消える。\n\\[ \\begin{align} \\mu_{1|1} \u0026amp;= \\hat{\\mu_1} + \\cfrac{P_1}{P_1 + \\sigma_v^2}(y_1 - \\hat{y}_1) \\\\\n\u0026amp;= \\mu_{0|0} + \\cfrac{P_{0|0} + \\sigma_w^2}{P_{0|0} + \\sigma_w^2 + \\sigma_v^2}(y_1 - \\mu_{0|0}) \\\\\n\u0026amp;= \\mu_{0|0} + y_1 - \\mu_{0|0} \\\\\n\u0026amp;= y_1 \\\\\n\\end{align} \\]\nまた、 \\(P_{1|1}\\) についても、以下のように定まる。\n\\[ \\begin{align} P_{1|1} \u0026amp;= (1-K_1)P_1 \\\\\n\u0026amp;= \\left(1 - \\cfrac{P_1}{P_1 + \\sigma_v^2} \\right)P_1 \\\\\n\u0026amp;= \\left(\\cfrac{\\sigma_v^2}{P_0 + \\sigma_w^2 + \\sigma_v^2}\\right)(P_0 + \\sigma_w^2) \\\\\n\u0026amp;= \\sigma_v^2 \\\\\n\\end{align} \\]\n平滑化 与えられた観測値を用いて、そのもととなる状態よりも過去の状態を補正することを平滑化という。平滑化によって補正された状態のことを 平滑化状態 と呼ぶ。\n平滑化のアイディア  予測が外れるのは過去の状態が間違っているからだ。予測が大きく外れるほどに、補正も大きくしよう。 前時点の状態について、不確かさが大きいのなら、補正も大きくしよう。 観測値の予測誤差が大きいなら、観測値は信頼できないので補正は小さくしよう。  従って、\n{平滑化状態} = {フィルタ化推定量} + {前時点の状態の分散}/{観測値の予測誤差} * {予測残差}\n平滑化の数式 最新の時刻を \\(T\\) とする。時刻 \\(t\\) における平滑化状態とその分散をそれぞれ \\(\\tilde{\\mu}_t\\) , \\(\\tilde{P}_t\\) とおく。これらは \\(T\\) までの全ての観測値 \\(Y_T\\) が得られた条件における、状態 \\(\\mu_t\\) の条件つき期待値と分散なので、\n\\[ \\begin{align} \\tilde{\\mu}_t \u0026amp;= \\mathrm{E}[\\mu_t|Y_T] \\\\\n\\tilde{P}_t \u0026amp;= \\mathrm{Var}[\\mu_t|Y_T] \\end{align} \\]\nと書ける。\nその他はカルマンフィルタの記述時の記号法に凖じる。\n例えば、 \\(T-1\\) 時点での平滑化推定量の計算式を数式に纏めると、\n\\[ \\tilde{\\mu}_{T-1} = \\mu_{T-1|T-1} + \\frac{P_{T-1|T-1}}{F_T}(y_T - \\hat{y}_T) \\]\n状態の予測値、フィルタ化推定量、平滑化状態 似て非なるものなので、少し整理する。\n\\(Y\\) の添字に注意。\n 状態の予測値:\n\\[ \\hat{\\mu}_t = \\mathrm{E}[\\mu_t|Y_{t-1}] \\]\n フィルタ化推定量:\n\\[ \\mu_{t|t} = \\mathrm{E}[\\mu_t|Y_t] \\]\n 平滑化状態:\n\\[ \\tilde{\\mu}_t = \\mathrm{E}[\\mu_t|Y_T] \\]\n  状態平滑化漸化式 平滑化状態の計算に用いる式。上に示した計算式では、任意の時刻 \\(t\\) における平滑化推定量の導出式にはなっていない。そこで、状態平滑化漸化式 \\(r_t\\) を導入して、一般形を得る。\n\\[\\begin{align}r_{t-1} \u0026amp;= \\frac{y_t - \\hat{y}_t}{F_t} + (1 - K_t)r_t \\\\\n\\tilde{\\mu}_t \u0026amp;= \\mu_{t|t} + P_{t|t}r_t \\end{align} \\]\nただし、 \\(r_t\\) は未来から過去に向かって計算するものとし、\n\\(r_T=0\\) とする。\n平滑化状態分散 \\(\\tilde{P}_t\\) も平滑化状態と同様に状態平滑化漸化式 \\(s_t\\) を用いて求められる。\n\\[ \\begin{align} s_{t-1} \u0026amp;= \\frac{1}{F_t} + (1-K_t)^2s_t \\\\\n\\tilde{P}_t \u0026amp;= P_{t|t} - P_{t|t}^2s_t \\end{align} \\]\nただし、 \\(s_T = 0\\)\nパラメタ推定 最尤推定によってパラメタ推定を行う。ここで推定されるパラメタは過程誤差の分散と観測誤差の分散である。\n観測値の予測残差 \\(y_t - \\hat{y}_t\\) を \\(d_t\\) とおく。\n\\(d_t\\) は正規分布に従うものと仮定する。\n\\[ d_t \\sim \\mathcal{N}(0, F_t) \\]\n\\(d_t\\) の確率密度関数 \\(f(d_t)\\) は、\n\\[ f(d_t) = \\frac{1}{\\sqrt{2\\pi F_t}}\\exp\\left(-\\frac{d_t^2}{2F_t}\\right) \\]\nこのとき、尤度を \\(L\\) とすると、\n\\[ L = \\prod_{t=1}^T f(d_t) = \\prod_{t=1}^T \\frac{1}{\\sqrt{2\\pi F_t}}\\exp\\left(-\\frac{d_t^2}{2F_t}\\right) \\]\n尤度の自然対数をとって、対数尤度関数とすると、\n\\[\\log{L} = -\\frac{T}{2}\\log{2\\pi} -\\frac{1}{2}\\sum_{t=1}^T\\left\\{ \\log{F_t} + \\frac{d_t^2}{F_t} \\right\\} \\]\n実際には、定数項は無視するので、第二項を正負反転させたものを最小化するパラメタを探す。\n\\[ \\frac{1}{2}\\sum_{t=1}^T\\left\\{\\log{F_t} + \\frac{d_t^2}{F_t} \\right\\} \\]\n"});})();